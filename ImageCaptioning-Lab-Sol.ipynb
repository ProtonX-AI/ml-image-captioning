{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImageCaptioning-Lab-Sol.ipynb","provenance":[{"file_id":"1bGnnkvFaOztRDEjTDu-KM3SNL50xdfNt","timestamp":1617857412509}],"collapsed_sections":["fQBxz8HVHKU2","XZYcbI62Ij8L","4iyLS49tJPLS","LH-OfnZ5K4qI","omGafG5YMvIp","c6FXPIDiMyEI"],"machine_shape":"hm","mount_file_id":"1bGnnkvFaOztRDEjTDu-KM3SNL50xdfNt","authorship_tag":"ABX9TyP1T6YgqngNThP0sZ21WD9e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"piAygHsDjWMA"},"source":["Speaker: Ba Ngoc - Founder @ProtonX @VietAI Hanoi"]},{"cell_type":"markdown","metadata":{"id":"-y5uR5I765f3"},"source":["## 1. Bổ sung các thư viện cần thiết"]},{"cell_type":"code","metadata":{"id":"dy53n084zvhm","cellView":"form"},"source":["#@title Cài đặt các thư viện cần thiết\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import collections\n","import random\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","from os import listdir\n","from os.path import isfile, join\n","\n","!pip install -q tqdm\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTMzaVOV7GJb"},"source":["#### Lựa chọn bộ dữ liệu"]},{"cell_type":"code","metadata":{"id":"Qe_5HwhpMAtJ","cellView":"form"},"source":["images_file_name = 'small_data'  #@param [\"tiny_data\", \"small_data\", \"large_data\"]\n","\n","mapping = {\n","'small_data': 'https://storage.googleapis.com/protonx-cloud-storage/images1000.zip',\n","'large_data' : 'https://storage.googleapis.com/protonx-cloud-storage/images5000.zip',\n","'tiny_data': 'https://storage.googleapis.com/protonx-cloud-storage/images500.zip'\n","}\n","\n","images_link = mapping[images_file_name]\n","captions_link = 'https://storage.googleapis.com/protonx-cloud-storage/annotations_trainval2014.zip'\n","\n","folder_mapping = {\n","'small_data': '/workshop_data_small/',\n","'large_data' : '/workshop_data/',\n","'tiny_data': '/workshop_data_tiny/'\n","}\n","\n","image_folder = folder_mapping[images_file_name]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfM-9AIh6_xa","cellView":"form"},"source":["#@title Tải về các miêu tả\n","# Tải các câu miêu tả\n","annotation_folder = '/annotations/'\n","if not os.path.exists(os.path.abspath('.') + annotation_folder):\n","  annotation_zip = tf.keras.utils.get_file('captions.zip',\n","                                          cache_subdir=os.path.abspath('.'),\n","                                          origin = captions_link,\n","                                          extract = True)\n","  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n","  os.remove(annotation_zip)\n","else:\n","  annotation_file = '/content/annotations/captions_train2014.json'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyRRErqy-FBW","cellView":"form"},"source":["#@title Tải về ảnh\n","\n","if not os.path.exists(os.path.abspath('.') + image_folder):\n","  image_zip = tf.keras.utils.get_file('images.zip',\n","                                      cache_subdir=os.path.abspath('.'),\n","                                      origin = images_link,\n","                                      extract = True)\n","  PATH = os.path.dirname(image_zip) + image_folder\n","  os.remove(image_zip)\n","else:\n","  PATH = os.path.abspath('.') + image_folder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ue5u_nZWCijO"},"source":["#### Đọc dữ liệu"]},{"cell_type":"code","metadata":{"id":"c4Hihgp1Csku","cellView":"form"},"source":["#@title Đọc file json chứa các miêu tả\n","\n","with open(annotation_file, 'r') as f:\n","    annotations = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opc6Fe4JCuRr","cellView":"form"},"source":["#@title Nối link ảnh và caption tương ứng\n","\n","image_files = [f for f in listdir(PATH) if isfile(join(PATH, f))]\n","aSet = set(image_files)\n","\n","\n","image_path_to_caption = collections.defaultdict(list)\n","for val in annotations['annotations']:\n","  caption = f\"<start> {val['caption']} <end>\"\n","  image_name = 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n","  if image_name in aSet:\n","    image_path = PATH + image_name\n","    image_path_to_caption[image_path].append(caption)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3jIVMmGRgrq"},"source":["# Kiểm tra số ảnh\n","len(image_path_to_caption)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTK2L4PxQeJ6"},"source":["Một link ảnh có thể có nhiều miêu tả"]},{"cell_type":"code","metadata":{"id":"3jjsSMkPEzC3","cellView":"form"},"source":["#@title Tráo ảnh\n","image_paths = list(image_path_to_caption.keys())\n","random.shuffle(image_paths)\n","\n","# Chọn số lượng ảnh mong muốn:\n","train_image_paths = image_paths[:]\n","print(len(train_image_paths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnBfqZjrFEYS","cellView":"form"},"source":["#@title Nối từng ảnh với từng miêu tả\n","# Duỗi thành 2 mảng chứa caption và ảnh tương ứng. Một bức ảnh có thể có nhiều caption miêu tả \\n\n","# Trường hợp này ta sẽ sắp xếp sử dụng 1 ảnh - 1 miêu tả\n","train_captions = []\n","img_name_vector = []\n","\n","for image_path in train_image_paths:\n","  caption_list = image_path_to_caption[image_path]\n","  train_captions.extend(caption_list)\n","  img_name_vector.extend([image_path] * len(caption_list)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYq7qKcEG-3p","cellView":"form"},"source":["#@title Hiển thị ảnh\n","idx = \"3\"  #@param [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n","print(train_captions[int(idx)])\n","Image.open(img_name_vector[int(idx)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQBxz8HVHKU2"},"source":["#### Xây dựng tensorflow dataset"]},{"cell_type":"code","metadata":{"id":"I1w1oCzhHXO-","cellView":"form"},"source":["#@title Tải về Inception V3\n","# Load Inceptionv3 chuẩn bị cho việc đưa các ảnh qua để sử dụng feature map hữu ích \n","\n","image_model = tf.keras.applications.InceptionV3(include_top=False,\n","                                                weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output # 8x8x2048\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8LBEiadEkhn","cellView":"form"},"source":["#@title Xây dựng TF dataset\n","# Hàm đọc ảnh. \n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (299, 299)) # Đổi chiều ảnh để phù hợp với yêu cầu của inceptionV3\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path\n","\n","\n","encode_train = sorted(set(img_name_vector))\n","\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(\n","  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img) # Đưa ảnh về feature maps (16, 8, 8, 2048)\n","  batch_features = tf.reshape(batch_features, \n","                              (batch_features.shape[0], -1, batch_features.shape[3])) # Nén chiều (16, 64, 2048)\n","  \n","  for bf, p in zip(batch_features, path): # Lưu dưới dạng .npy\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZYcbI62Ij8L"},"source":["#### Tiền xử lý và tokenize miêu tả - TODO"]},{"cell_type":"code","metadata":{"id":"719xA0P5IDcR","cellView":"form"},"source":["#@title Tìm chiều dài lớn nhất của bất kỳ một caption nào trong dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmeVpz6eIyJf"},"source":["#@title Chuyển văn bản thành từ điển\n","top_k = 5000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","\n","#### TODO 1: Xây dựng từ điển\n","tokenizer.fit_on_texts(train_captions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHW4TQJPI35m","cellView":"form"},"source":["#@title Thêm padding cho từ điển\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33y9igsXI6sN"},"source":["#### TODO 2: Chuyển miêu tả thành sequence \n","train_seqs = tokenizer.texts_to_sequences(train_captions) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-uemgj-I9ef"},"source":["#### TODO 3: Padding để đảm bảo câu có cùng chiều dài\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hd1LFZlAJEGr","cellView":"form"},"source":["#@title Tính chiều dài lớn nhất của câu\n","max_length = calc_max_length(train_seqs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iyLS49tJPLS"},"source":["#### Chia tập test và tập train"]},{"cell_type":"code","metadata":{"id":"CtT34By6JNwf","cellView":"form"},"source":["#@title Chia tập Test và tập train\n","\n","img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(img_name_vector, cap_vector):\n","  img_to_cap_vector[img].append(cap)\n","\n","# Create training and validation sets using an 80-20 split randomly.\n","img_keys = list(img_to_cap_vector.keys())\n","random.shuffle(img_keys)\n","\n","slice_index = int(len(img_keys)*0.8)\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","img_name_train = []\n","cap_train = []\n","for imgt in img_name_train_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  img_name_train.extend([imgt] * capt_len)\n","  cap_train.extend(img_to_cap_vector[imgt])\n","\n","img_name_val = []\n","cap_val = []\n","for imgv in img_name_val_keys:\n","  capv_len = len(img_to_cap_vector[imgv])\n","  img_name_val.extend([imgv] * capv_len)\n","  cap_val.extend(img_to_cap_vector[imgv])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaHvsjloVO4N"},"source":["len(img_name_train), len(img_name_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASP2rUneKumW"},"source":["# Định nghĩa các siêu tham số (hyper parameter) của mô hình\n","\n","#### TODO 4: Số lượng data được sử dụng trong một lần đưa vào mô hình\n","BATCH_SIZE = 64 # ADD TODO HERE\n","\n","BUFFER_SIZE = 1000 \n","\n","# Chiều khi đưa qua embedding\n","embedding_dim = 256 # ADD TODO HERE\n","\n","#### TODO 5: Chiều của hidden state \n","units = 512 # ADD TODO HERE\n","\n","# Số lượng từ trong từ điển\n","vocab_size = top_k + 1\n","\n","# Số bước training \n","num_steps = len(img_name_train) // BATCH_SIZE # ADD TODO HERE\n","\n","# Chiều của features shape\n","features_shape = 2048 # ADD TODO HERE\n","\n","# Chiều của attention\n","attention_features_shape = 64\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Gli0S0PKzYl","cellView":"form"},"source":["#@title Tải ảnh vào Dataset\n","# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# Load ảnh từ file\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LH-OfnZ5K4qI"},"source":["#### Mô hình"]},{"cell_type":"markdown","metadata":{"id":"CTUzT4GLK8Po"},"source":["Cơ chế Attention tìm những kết nối giữa từ và vùng ảnh quan hệ mật thiết với nhau"]},{"cell_type":"code","metadata":{"id":"cQntgHguK4UN","cellView":"form"},"source":["#@title Cơ chế Attention\n","# Sử dụng các hidden phía trước để thực hiện kết nối với những khu vực ảnh quan trọng (features từ đầu ra của CNN)\n","# Vector hidden này bao gồm thông tin quan trọng từ đầu câu đến vị trí phía trước\n","\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","\n","    # 1. Chiều của features: (batch_size, 64, embedding_dim)\n","    # 2. Chiều của hidden: (batch_size, units)\n","    # 3. Mở rộng chiều hidden_with_time_axis: (batch_size, 1, units)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","\n","    # 1. Biến đổi tuyến tính features về chiều (batch_size, 64, units)\n","    # 2. Biến đổi hidden_with_time_axis về chiều (batch_size, 1, units)\n","    # 3. Broadcasting hidden_with_time_axis vào features. \n","    # 4. Đầu ra attention_hidden_layer: (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                         self.W2(hidden_with_time_axis)))\n","\n","\n","    # 1. Đưa qua biến đổi tuyến tính từ (batch_size, 64, units) về (batch_size, 64, 1)\n","    # 2. score này thể hiện 64 mối quan hệ của vector hidden với 64 khu vực ảnh\n","    score = self.V(attention_hidden_layer)\n","\n","    \n","    # 1. Thực hiện softmax để tìm ra xác suất khu vực quan trọng. \n","    # 2. Tổng các giá trị này sẽ bằng 1 \n","    # 3. Chiều của attention_weights: (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","\n","    # 1. Bổ sung mối quan hệ này vào các khu vực hiện tại và tổng hợp lại thành một vector\n","    # 2. Chiều của context_vector: (batch_size, embedding_dim)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","    \n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yv12ZlTOl1iM","cellView":"form"},"source":["#@title Kiểm tra chiều Attention\n","\n","def checkShape(var):\n","  print('Shape: {}'.format(var.shape))\n","\n","batch_size = 16\n","att = BahdanauAttention(units)\n","features = tf.ones((batch_size, 64, embedding_dim))\n","hidden = tf.ones((batch_size, units))\n","context_vector, attention_weights = att(features, hidden)\n","print('Chiều của từ được bổ sung liên kết - context vector {}'.format(context_vector.shape))\n","print('Chiều ma trận quan hệ attention_weights {}'.format(attention_weights.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ciPT3hMLlKD","cellView":"form"},"source":["#@title Kiến trúc Encoder - Đọc hình ảnh\n","class CNN_Encoder(tf.keras.Model):\n","\n","    # Đưa các feature map đã trích xuất từ inceptionv3 qua một mạng nơ ron\n","\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        # Đầu vào: (batch_size, 64, features_shape)\n","        x = self.fc(x)\n","        x = tf.nn.relu(x) # Đầu ra của mạng nơ ron: (batch_size, 64, embedding_dim)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRKeIlwwLyMW","cellView":"form"},"source":["#@title Kiến trúc Decoder - Dự đoán từ\n","class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # 1. Thực hiện attention hidden state và các vùng ảnh và tìm ra \n","    # các trọng số quan hệ attention_weights và vector được bổ sung quan hệ\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # 1. Đưa một từ qua Embedding để trích xuất vector thông tin \n","    # 2. Chiều của vector embedding (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # 1. Nối x và context_vector để tiến hành dự đoán\n","    # 2. x chứa thông tin của từ hiện tại và mối quan hệ của hidden state phía trước và các khu vực ảnh\n","    # 3. Chiều mới của x. (batch_size, 1, embedding_dim) + (batch_size, 1, embedding_dim) = (batch_size, 1, 2 * embedding_dim)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # 1. Đưa x qua mạng GRU để sinh ra hidden hiện tại\n","    # 2. Chiều của output: (batch_size, 1, units). (Toàn bộ các state ở các timestep)\n","    # 3. Chiều của state: (batch_size, 1, units). \n","    output, state = self.gru(x)\n","\n","    # 1. Đưa x qua mạng nơ ron \n","    # 2. Chiều của x: (batch_size, 1, units)\n","    x = self.fc1(output)\n","\n","    # 1. Nén chiều của x từ (batch_size, 1, units) thành (batch_size * 1, units)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # 1. Tiếp tục đưa qua mạng nơ ron. \n","    # 2. Chiều của x: (batch_size * 1, units) -> (batch_size * 1, vocab_size)\n","    x = self.fc2(x)\n","\n","\n","    # Chiều của đầu ra:\n","    # x: (batch_size * 1, vocab_size)\n","    # hidden state: (batch_size, units)\n","    # attention_weights: (batch_size, 64, 1)\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVkiLPljLxYm"},"source":["# Khởi tạo nhưng chưa chạy - Encoder/Decoder\n","\n","#### TODO 6: Khởi tạo Encoder\n","encoder = CNN_Encoder(embedding_dim)\n","\n","\n","#### TODO 7: Khởi tạo Decoder\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size) # Add TODO HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T__ZXhSGUOGD","cellView":"form"},"source":["#@title Kiểm tra chiều encoder\n","image_features = tf.ones((16, 64, features_shape))\n","encoder_output = encoder(image_features)\n","\n","# Expected Shape: (16, 64, embedding_dim) = (16, 64, 256)\n","# Real Shape: \n","features = encoder(image_features)\n","print('Chiều của các vùng ảnh quan trọng được trích xuất từ Inception V3: {}'.format(features.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTQk0uLVUs6b","cellView":"form"},"source":["#@title Kiểm tra chiều decoder\n","hidden = tf.ones((16, units))\n","features = tf.ones((16, 64, embedding_dim))\n","w = tf.ones((16, 1))\n","x, state, attention_weights = decoder(w, features, hidden)\n","\n","print('Vector sử dụng để dự đoán từ tiếp theo: {}'.format(x.shape))\n","print('Hidden state hiện tại: {}'.format(state.shape))\n","print('Ma trận thể hiện mối quan hệ giữa hidden state phía trước với các khu vực ảnh attention_weights: {}'.format(attention_weights.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omGafG5YMvIp"},"source":["#### Cài đặt thuật toán"]},{"cell_type":"code","metadata":{"id":"NsYjOl-nMb5t","cellView":"form"},"source":["#@title Cài đặt thuật toán training: Adam. Hàm mất mát: CrossEntropy\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6FXPIDiMyEI"},"source":["#### Lưu trữ checkpoint"]},{"cell_type":"code","metadata":{"id":"lLeknQayULNr","cellView":"form"},"source":["#@title Tải về và load checkpoint đã được train trước\n","\n","checkpoint_folder = '/check_point/'\n","checkpoint_path = 'https://storage.googleapis.com/protonx-cloud-storage/check_point.zip'\n","\n","if not os.path.exists(os.path.abspath('.') + checkpoint_folder):\n","  checkpoint_zip = tf.keras.utils.get_file('check_point.zip',\n","                                          cache_subdir=os.path.abspath('.'),\n","                                          origin = checkpoint_path,\n","                                          extract = True)\n","  os.remove(checkpoint_zip)\n","\n","checkpoint_path = \"/content/check_point/\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # Sử dụng tham số đã được train từ trước\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwqKbTNLM1Tk"},"source":["#### Định nghĩa bước training"]},{"cell_type":"code","metadata":{"id":"VPeUUX3tMpac","cellView":"code"},"source":["#@title Bước Training - TODO\n","\n","@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # 1. Khởi tạo hidden state. Chiều: (batch_size, 512)\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  # 2. Tạo từ xuất phát dự đoán với token mở đầu là: <start>. Chiều: (batch_size, 1)\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","  # 3. Sử dụng Gradient Tape để tính lan truyền ngược sau này\n","  \n","  with tf.GradientTape() as tape:\n","\n","      # 4. Lấy features hữu tích từ encoder ảnh: \n","      \n","      #### TODO 8: Chạy Encoder\n","      features = encoder(img_tensor) # 5. Chiều: (batch_size, 64, embedding_dim)\n","\n","      # 6. Lặp từ vị trí tiếp theo đến hết câu trong nhãn \n","      \n","      for i in range(1, target.shape[1]):\n","          \n","          # 7. Đưa features và hidden phía trước cùng với từ hiện tại đi qua decoder\n","          # 8. Kết quả thu được dự đoán từ tiếp theo và hidden hiện tại\n","\n","          predictions, hidden, _ = decoder(dec_input, features, hidden) #### TODO 9: Chạy Decoder\n","\n","          # 9. Tính sai lệch giữa kết quả dự đoán và nhãn thật\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # 10. Vì là quá trình training cho nên không sử dụng từ dự đoán ra để đưa vào \n","          # mô hình mà tiếp tục sử dụng từ tiếp theo trong nhãn. Quá trình này còn được gọi là teacher forcing\n","\n","          dec_input = tf.expand_dims(target[:, i], 1) # Chiều (batch_size, 1)\n","\n","  # 11. Tính loss trung bình\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  # 12. Lấy ra các tham số cần cập nhật của mô hình\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  # 13. Lấy ra gradient của loss theo những tham số này\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  # 14. Cập nhật giá trị tham số thông qua gradient.\n","  optimizer.apply_gradients(zip(gradients, trainable_variables)) # #### TODO 10: Cập nhật tham số\n","\n","  return loss, total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohWMagv9MrXF","cellView":"code"},"source":["#@title Tiến hành Training - TODO\n","loss_plot = []\n","\n","EPOCHS = 10\n","\n","# 1. Lặp qua các epoch: Epoch có thể coi là số lần lặp qua toàn bộ tập dữ liệu và cập nhật tham số\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","    \n","    # 2. Trong mỗi epoch chia thành nhiều phần nhỏ, và tiến hành train trên các phần nhỏ này\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","        batch_loss, t_loss = train_step(img_tensor, target)  #### TODO 11: Cài đặt bước training\n","        \n","        # 3. Cộng dồn mất mát trên các phần nhỏ này\n","        total_loss += t_loss\n","\n","        # 4. In mất mát\n","        if batch % 100 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n","              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","    \n","    loss_plot.append(total_loss / num_steps)\n","\n","    if epoch % 5 == 0:\n","      ckpt_manager.save()\n","\n","    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n","                                         total_loss/num_steps))\n","    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WLNrIR50NHTJ"},"source":["#### Tiến hành dự đoán"]},{"cell_type":"code","metadata":{"id":"uhsHEniNM-Fq","cellView":"code"},"source":["#@title Tiến hành dự đoán - TODO\n","def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    # 1. Khởi tạo hidden state\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    # 2. Đưa ảnh đầu vào qua InceptionV3\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n","                                                 -1,\n","                                                 img_tensor_val.shape[3]))\n","    # 3. Encode ảnh \n","    features = encoder(img_tensor_val)\n","\n","    # 4. Định nghĩa từ mồi: <start>\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    result = []\n","\n","    # 5. Lặp qua chiều dài nhất có thể của câu. \n","    # 5.1. Trong quá trình dự đoán nếu gặp token <end>: phá vỡ vòng for\n","    # 5.2. Nếu không gặp token <end> thì câu dự đoán có chiều dài lớn nhất bằng max_length\n","\n","    for i in range(max_length):\n","\n","        # 6. Đưa từ, features của ảnh và hidden state phía trước đi qua decoder \n","        predictions, hidden, attention_weights = decoder(dec_input,\n","                                                         features,\n","                                                         hidden) \n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        # 7. Dự đoán token của từ tiếp theo\n","        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        # 8. Nếu gặp token <end> thì phá vỡ vòng lặp\n","        #### TODO 12: Xử lý khi gặp token <end>\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot\n","\n","        # 9. Gán lại từ đầu vào để sử dụng cho lần tiếp theo\n","        #### TODO 13: Sử dụng từ cho lần dự đoán tiếp\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAbhIdBDNMAm","cellView":"form"},"source":["#@title Hàm hiển thị sự liên kết\n","def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for i in range(len_result):\n","        temp_att = np.resize(attention_plot[i], (8, 8))\n","        grid_size = max(np.ceil(len_result/2), 2)\n","        ax = fig.add_subplot(grid_size, grid_size, i+1)\n","        ax.set_title(result[i])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oedGJ57NNQh","cellView":"form"},"source":["#@title Test trên bộ validation\n","\n","rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","real_caption = ' '.join([tokenizer.index_word[i]\n","                        for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","Image.open(img_name_vector[rid])\n","print('Real Caption:', real_caption)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4Ngbqn-NWfF"},"source":["#### Sử dụng một ảnh bên ngoài"]},{"cell_type":"code","metadata":{"id":"Q_wJ9667NWJJ","cellView":"form"},"source":["#@title Chèn một link ảnh có sẵn vào đây\n","link = \"https://static.scientificamerican.com/sciam/cache/file/9097719F-9951-42CC-AC84E68BFCB50358_source.jpg\" #@param {type:\"string\"}\n","\n","image_url = link if link else 'https://tensorflow.org/images/surf.jpg'\n","image_extension = image_url[-4:]\n","image_name = image_url.split('/')[-1][:-4]\n","image_path = tf.keras.utils.get_file(image_name + image_extension, origin=image_url)\n","\n","result, attention_plot = evaluate(image_path)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image_path, result, attention_plot)\n","# opening the image\n","Image.open(image_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdE93ab_QF41"},"source":["#### Sử dụng file được tải từ máy tính"]},{"cell_type":"code","metadata":{"id":"bnKRawapNo4O","cellView":"form"},"source":["#@title Tải ảnh lên và tiến hành dự đoán\n","from google.colab import files\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","  \n","  image = plt.imread(fn)\n","  plt.imshow(image)\n","\n","  result, attention_plot = evaluate(fn)\n","  print('Prediction Caption:', ' '.join(result))\n","  plot_attention(fn, result, attention_plot)\n","  # opening the image\n","  Image.open(image_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cd-iQOU9JWs"},"source":["# Testing link: https://static.scientificamerican.com/sciam/cache/file/9097719F-9951-42CC-AC84E68BFCB50358_source.jpg"],"execution_count":null,"outputs":[]}]}